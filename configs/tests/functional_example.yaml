# Example functional suite config for vLLM (enable by pointing env var):
#   export VLLM_CIBENCH_FUNCTIONAL_CONFIG=$(pwd)/configs/tests/functional_example.yaml
# Or copy to configs/tests/functional.yaml and set suite: true

enabled: true
suite: true

cases:
  # Basic chat
  - id: chat_basic
    type: chat
    messages:
      - role: user
        content: "Say hello in one word."
    params:
      temperature: 0

  # Basic completions
  - id: comp_basic
    type: completions
    prompt: "Hello"
    params:
      max_tokens: 8

matrices:
  chat:
    - id_prefix: chat_bounds
      messages:
        - role: user
          content: "Summarize: vLLM is great."
      params_grid:
        temperature: [0.0, 1.0]
        top_p: [0.0, 1.0]
        top_k: [1, 8]
      expect_error: false
    - id_prefix: chat_stream_json
      messages:
        - role: user
          content: "Return JSON with key 'ok' true."
      params_grid:
        stream: [true]
        response_format: [{type: json_object}]
      expect_error: false
  completions:
    - id_prefix: comp_bounds
      prompt: "Say X"
      params_grid:
        logprobs: [false, true]
        top_logprobs: [1, 3]
      expect_error: false

negative:
  chat:
    - id_prefix: chat_invalid_top_p
      messages:
        - role: user
          content: "hi"
      params_list:
        - {top_p: 1.5}
      expect_error: true
    - id_prefix: chat_n_gt_best_of_stream
      messages:
        - role: user
          content: "hi"
      params_list:
        - {n: 2, best_of: 1, stream: true}
      expect_error: true
  completions:
    - id_prefix: comp_logprobs_bounds
      prompt: "hello"
      params_list:
        - {logprobs: true, top_logprobs: 0}
        - {logprobs: true, top_logprobs: 100}
      expect_error: true

