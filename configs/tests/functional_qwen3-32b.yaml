enabled: true
suite: true

functional_metrics:
  per_case: true

capabilities:
  - chat.tools
  - chat.tool_choice
  - chat.tool_choice.by_name
  - chat.response_format.json_object
  - chat.response_format.json_schema
  - chat.logprobs
  - completions.suffix
  - completions.best_of

cases:
  - id: chat_basic
    type: chat
    messages:
      - role: user
        content: "Say hello in one word."
    params:
      temperature: 0

  - id: chat_json_object
    type: chat
    messages:
      - role: user
        content: "Return a JSON object with key 'ok'=true."
    params:
      response_format: {type: json_object}
    required_capabilities: [chat.response_format.json_object]

  - id: chat_json_schema_with_tools
    type: chat
    messages:
      - role: user
        content: "Get weather for Beijing and return JSON with keys city,temp_c."
    params:
      tools:
        - type: function
          function:
            name: get_weather
            description: Get weather by city
            parameters:
              type: object
              properties: {city: {type: string}}
              required: [city]
      tool_choice: required
      response_format:
        type: json_schema
        json_schema:
          name: Weather
          schema:
            type: object
            properties:
              city: {type: string}
              temp_c: {type: number}
            required: [city, temp_c]
    required_capabilities: [chat.tools, chat.response_format.json_schema]

  - id: chat_tools_named_required
    type: chat
    messages:
      - role: user
        content: "Get weather for Beijing"
    params:
      tools:
        - type: function
          function:
            name: get_weather
            description: Get weather by city
            parameters:
              type: object
              properties: {city: {type: string}}
              required: [city]
      tool_choice: {type: function, function: {name: get_weather}}
    required_capabilities: [chat.tool_choice.by_name]

  - id: chat_stream_basic
    type: chat
    messages:
      - role: user
        content: "Greet me."
    params:
      stream: true
      temperature: 0

  - id: chat_multiturn_stream_tools
    type: chat
    messages:
      - role: system
        content: "You are helpful."
      - role: user
        content: "What's the weather in Beijing?"
    params:
      tools:
        - type: function
          function:
            name: get_weather
            parameters:
              type: object
              properties: {city: {type: string}}
              required: [city]
      tool_choice: required
      stream: true
    required_capabilities: [chat.tools]

  - id: comp_basic
    type: completions
    prompt: "Hello"
    params:
      max_tokens: 8

matrices:
  chat:
    - id_prefix: chat_bounds
      messages:
        - role: user
          content: "Summarize: vLLM is great."
      params_grid:
        temperature: [0.0, 1.0]
        top_p: [0.0, 1.0]
        top_k: [1, 8]
      expect_error: false
    - id_prefix: chat_bounds_stop
      messages:
        - role: user
          content: "Say A B C"
      params_grid:
        stop: [["B"]]
        max_tokens: [4, 8]
      expect_error: false
  completions:
    - id_prefix: comp_bounds
      prompt: "Say X"
      params_grid:
        logprobs: [false, true]
        top_logprobs: [1, 3]
      expect_error: false

negative:
  chat:
    - id_prefix: chat_invalid_top_p
      messages:
        - role: user
          content: "hi"
      params_list:
        - {top_p: 1.5}
      expect_error: true
    - id_prefix: chat_n_gt_best_of_stream
      messages:
        - role: user
          content: "hi"
      params_list:
        - {n: 2, best_of: 1, stream: true}
      expect_error: true
    - id_prefix: chat_negative_temperature
      messages:
        - role: user
          content: "hi"
      params_list:
        - {temperature: -0.1}
      expect_error: true
    - id_prefix: chat_invalid_stop_token_ids
      messages:
        - role: user
          content: "hi"
      params_list:
        - {stop_token_ids: ["bad"]}
      expect_error: true
  completions:
    - id_prefix: comp_logprobs_bounds
      prompt: "hello"
      params_list:
        - {logprobs: true, top_logprobs: 0}
        - {logprobs: true, top_logprobs: 100}
      expect_error: true
    - id_prefix: comp_negative_max_tokens
      prompt: "hello"
      params_list:
        - {max_tokens: -1}
      expect_error: true
    - id_prefix: comp_invalid_echo_type
      prompt: "hello"
      params_list:
        - {echo: "true"}
      expect_error: true
